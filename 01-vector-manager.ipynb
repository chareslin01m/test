{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d75ceac",
   "metadata": {},
   "source": [
    "# GitHub RAG - Vector Manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30da8ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8b6d4",
   "metadata": {},
   "source": [
    "LangChain 추적\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eaca08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG - Vector Manager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da2f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be42ea",
   "metadata": {},
   "source": [
    "### 데이터 준비\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a6b37",
   "metadata": {},
   "source": [
    "GitHub Repository 를 다운로드 받습니다. 이번 예제는 langchain-ai 공식 Repository 로 진행합니다.\n",
    "\n",
    "- !git clone 을 사용하여 Repository clone\n",
    "\n",
    "다른 저장소의 파일을 사용하고 싶다면, `root_dir`을 여러분의 저장소의 루트 디렉토리로 변경하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e5414",
   "metadata": {},
   "source": [
    "저는 `/Users/teddy/Dev/github/langchain` 위치에 `langchain` reposotory 를 clone 하였습니다. 아래의 경로는 본인의 경로에 맞게 바꾸어 주어야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1390039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli\t   core\t\t langchain  standard-tests\n",
      "community  experimental  partners   text-splitters\n"
     ]
    }
   ],
   "source": [
    "#!ls \"/Users/teddy/Dev/github/langchain/libs\"\n",
    "!ls \"/home/charles/ai_project/langchain/libs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc53e2",
   "metadata": {},
   "source": [
    "## 도큐먼트 로더\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2359dde",
   "metadata": {},
   "source": [
    "여기서 모든 패키지의 파일을 불러오지 않습니다. 핵심 기능을 포함하는 특정 폴더의 파일만 불러오도록 아래와 같이 정의해 주었습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1330d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root 경로\n",
    "# repo_root = \"/Users/teddy/Dev/github/langchain/libs\"\n",
    "# repo_root = \"/Users/julio/projects/gpt-projects/langchain/libs\"\n",
    "repo_root = \"/home/charles/ai_project/langchain/libs\"\n",
    "\n",
    "# 불러오고자 하는 패키지 경로\n",
    "repo_core = repo_root + \"/core/langchain_core\"\n",
    "repo_community = repo_root + \"/community/langchain_community\"\n",
    "repo_experimental = repo_root + \"/experimental/langchain_experimental\"\n",
    "repo_parters = repo_root + \"/partners\"\n",
    "repo_text_splitter = repo_root + \"/text_splitters/langchain_text_splitters\"\n",
    "# repo_cookbook = repo_root + \"/cookbook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b57bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:54: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:54: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:54: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:321: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<unknown>:321: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<unknown>:321: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<unknown>:147: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:147: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:147: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\s'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py 파일의 개수: 5830\n"
     ]
    }
   ],
   "source": [
    "# langchain의 여러 모듈을 가져옵니다.\n",
    "from langchain_text_splitters import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "\n",
    "# 불러온 문서를 저장할 빈 리스트를 생성합니다.\n",
    "py_documents = []\n",
    "\n",
    "for path in [repo_core, repo_community, repo_experimental, repo_parters]:\n",
    "    # GenericLoader를 사용하여 파일 시스템에서 문서를 로드합니다.\n",
    "    loader = GenericLoader.from_filesystem(\n",
    "        path,  # 문서를 불러올 경로\n",
    "        glob=\"**/*\",  # 모든 하위 폴더와 파일을 대상으로 함\n",
    "        suffixes=[\".py\"],  # .py 확장자를 가진 파일만 대상으로 함\n",
    "        parser=LanguageParser(\n",
    "            language=Language.PYTHON, parser_threshold=30\n",
    "        ),  # 파이썬 언어의 문서를 파싱하기 위한 설정\n",
    "    )\n",
    "    # 로더를 통해 불러온 문서들을 documents 리스트에 추가합니다.\n",
    "    py_documents.extend(loader.load())\n",
    "\n",
    "print(f\".py 파일의 개수: {len(py_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784611ea",
   "metadata": {},
   "source": [
    "다음은 `.mdx` 확장자를 가진 파일을 `TextLoader` 를 사용하여 불러옵니다. `.mdx` 파일은 Jupyter Notebook 파일을 마크다운 형식으로 변환한 파일이며, 유용한 예제를 포함하고 있으므로 이를 DB 에 추가하기 위해 도큐먼트 형식으로 로드압니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3c3412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".mdx 파일의 개수: 272\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# TextLoader 모듈을 불러옵니다.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 검색할 최상위 디렉토리 경로를 정의합니다.\n",
    "# root_dir = \"/Users/teddy/Dev/github/langchain/\"\n",
    "root_dir = \"/home/charles/ai_project/langchain/\"\n",
    "\n",
    "\n",
    "mdx_documents = []\n",
    "# os.walk를 사용하여 root_dir부터 시작하는 모든 디렉토리를 순회합니다.\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # 각 디렉토리에서 파일 목록을 확인합니다.\n",
    "    for file in filenames:\n",
    "        # 파일 확장자가 .mdx인지 확인하고, 경로 내 '*venv/' 문자열이 포함되지 않는지도 체크합니다.\n",
    "        if (file.endswith(\".mdx\")) and \"*venv/\" not in dirpath:\n",
    "            try:\n",
    "                # TextLoader를 사용하여 파일의 전체 경로를 지정하고 문서를 로드합니다.\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                # 로드한 문서를 분할하여 documents 리스트에 추가합니다.\n",
    "                mdx_documents.extend(loader.load())\n",
    "            except Exception:\n",
    "                # 파일 로드 중 오류가 발생하면 이를 무시하고 계속 진행합니다.\n",
    "                pass\n",
    "\n",
    "# 최종적으로 불러온 문서의 개수를 출력합니다.\n",
    "print(f\".mdx 파일의 개수: {len(mdx_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff91cf1",
   "metadata": {},
   "source": [
    "## Chunk 분할\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec8689",
   "metadata": {},
   "source": [
    "파일들을 청크로 나누어 봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da24fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 .py 파일의 개수: 17103\n",
      "분할된 .mdx 파일의 개수: 1092\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter 모듈을 가져옵니다.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# RecursiveCharacterTextSplitter 객체를 생성합니다. 이 때, 파이썬 코드를 대상으로 하며,\n",
    "# 청크 크기는 2000, 청크간 겹치는 부분은 200 문자로 설정합니다.\n",
    "py_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "# py_docs 변수에 저장된 문서들을 위에서 설정한 청크 크기와 겹치는 부분을 고려하여 분할합니다.\n",
    "py_docs = py_splitter.split_documents(py_documents)\n",
    "\n",
    "# 분할된 텍스트의 개수를 출력합니다.\n",
    "print(f\"분할된 .py 파일의 개수: {len(py_docs)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mdx_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# mdx_docs 변수에 저장된 문서들을 위에서 설정한 청크 크기와 겹치는 부분을 고려하여 분할합니다.\n",
    "mdx_docs = mdx_splitter.split_documents(mdx_documents)\n",
    "\n",
    "# 분할된 텍스트의 개수를 출력합니다.\n",
    "print(f\"분할된 .mdx 파일의 개수: {len(mdx_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0fc01",
   "metadata": {},
   "source": [
    "로드한 도큐먼트를 합칩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a622a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 도큐먼트 개수: 18195\n"
     ]
    }
   ],
   "source": [
    "combined_documents = py_docs + mdx_docs\n",
    "print(f\"총 도큐먼트 개수: {len(combined_documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b314197",
   "metadata": {},
   "source": [
    "## [추가됨] Combind Document to Local File(Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ab7a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents saved to ./documents/combined_documents.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# 문서들을 로컬 파일에 저장하는 함수\n",
    "def save_documents_to_file(documents, file_path):\n",
    "    # 파일 경로의 디렉토리를 추출\n",
    "    directory = os.path.dirname(file_path)\n",
    "    \n",
    "    # 디렉토리가 존재하지 않으면 생성\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    print(f\"Documents saved to {file_path}\")\n",
    "\n",
    "# 문서들을 로컬 파일에 저장\n",
    "save_documents_to_file(combined_documents, './documents/combined_documents.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4fe6f7",
   "metadata": {},
   "source": [
    "## Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59281459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_openai와 langchain의 필요한 모듈들을 가져옵니다.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "# 로컬 파일 저장소를 사용하기 위해 LocalFileStore 인스턴스를 생성합니다.\n",
    "# './cache/' 디렉토리에 데이터를 저장합니다.\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# OpenAI 임베딩 모델 인스턴스를 생성합니다. 모델명으로 \"text-embedding-3-small\"을 사용합니다.\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", disallowed_special=())\n",
    "\n",
    "# CacheBackedEmbeddings를 사용하여 임베딩 계산 결과를 캐시합니다.\n",
    "# 이렇게 하면 임베딩을 여러 번 계산할 필요 없이 한 번 계산된 값을 재사용할 수 있습니다.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, store, namespace=embeddings.model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e23a9d",
   "metadata": {},
   "source": [
    "## Vector DB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45d0ff",
   "metadata": {},
   "source": [
    "- `from_documents`: 이 메서드는 문서 컬렉션과 이에 해당하는 임베딩을 받아 벡터 인덱스를 생성합니다. 여기서는 `combined_documents` 로부터 벡터를 생성하고, 이 벡터들은 `cached_embeddings` 을 통해 임베딩된 데이터를 사용합니다.\n",
    "- `save_local`: 이 메서드는 생성된 FAISS 인덱스를 지정된 로컬 폴더에 저장합니다. 이 폴더명은 `FAISS_DB_INDEX` 변수에 저장되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a6be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_community 모듈에서 FAISS 클래스를 가져옵니다.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 로컬에 저장할 FAISS 인덱스의 폴더 이름을 지정합니다.\n",
    "FAISS_DB_INDEX = \"langchain_faiss\"\n",
    "\n",
    "# combined_documents 문서들과 cached_embeddings 임베딩을 사용하여\n",
    "# FAISS 데이터베이스 인스턴스를 생성합니다.\n",
    "db = FAISS.from_documents(combined_documents, cached_embeddings)\n",
    "\n",
    "# 생성된 데이터베이스 인스턴스를 지정한 폴더에 로컬로 저장합니다.\n",
    "db.save_local(folder_path=FAISS_DB_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bc77b",
   "metadata": {},
   "source": [
    "FAISS 데이터베이스 및 Document의 저장이 정상적으로 완료되었습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
